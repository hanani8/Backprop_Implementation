{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLwVktOPl95ijFtTM2ADFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanani8/Backprop_Implementation/blob/main/BACKPROP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "LxKsL2ohbBI3"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "\n",
        "no_of_layers = 2\n",
        "no_of_inputs = 3\n",
        "no_of_outputs = 3\n",
        "no_of_neurons_in_each_layer = 3\n",
        "X = np.array([[1], [0], [1]])\n",
        "Y = np.array([[0], [0], [1]])\n"
      ],
      "metadata": {
        "id": "x_HFSoIRcf3D"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiallary Functions\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    denominator = np.sum(np.exp(i) for i in x)\n",
        "    return np.array([ (np.exp(i) / denominator) for i in x ])\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def linear(w,x,b):\n",
        "  return np.sum(np.dot(w, x), b)\n",
        "\n",
        "def error(h):\n",
        "  return -np.sum(Y * np.log(h[-1]))"
      ],
      "metadata": {
        "id": "ckcm1969bD1O"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameter Initialization\n",
        "\n",
        "W = []\n",
        "B = []\n",
        "\n",
        "for i in range(no_of_layers + 1):\n",
        "\n",
        "  if i != no_of_layers:\n",
        "\n",
        "    W.append(np.zeros(no_of_neurons_in_each_layer ** 2).reshape(no_of_neurons_in_each_layer, no_of_neurons_in_each_layer))\n",
        "    B.append(np.zeros(no_of_neurons_in_each_layer))\n",
        "\n",
        "  else:\n",
        "\n",
        "    W.append(np.zeros(no_of_neurons_in_each_layer * no_of_outputs).reshape(no_of_neurons_in_each_layer, no_of_outputs))\n",
        "    B.append(np.zeros(no_of_outputs))\n",
        "\n",
        "for i in W:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEFbw2eGbeCB",
        "outputId": "e334d779-2ad7-46a9-8b33-f6b1b80f36cb"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Weights and Biases\n",
        "files = np.load('/content/parameters.npz')\n",
        "for i in range(1, no_of_layers + 2):\n",
        "  W[i-1] = files.get(\"W{}\".format(i))\n",
        "  B[i-1] = files.get(\"b{}\".format(i))"
      ],
      "metadata": {
        "id": "DM6ol3G-XuFg"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Propagation\n",
        "\n",
        "def forward_propagation(input):\n",
        "\n",
        "  a = []\n",
        "  h = []\n",
        "\n",
        "  for layer in range(no_of_layers + 1):\n",
        "\n",
        "    if layer == 0:\n",
        "      a.append( W[layer] @ input + B[layer] )\n",
        "    else:\n",
        "      a.append( W[layer] @ h[layer - 1] + B[layer])\n",
        "\n",
        "    if layer != no_of_layers:\n",
        "\n",
        "      h.append(sigmoid(a[layer]))\n",
        "\n",
        "    else:\n",
        "\n",
        "      h.append(softmax(a[layer]))\n",
        "\n",
        "  return (a,h)"
      ],
      "metadata": {
        "id": "-ttlKRgxbYjV"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Backward Proagation\n",
        "\n",
        "def backward_propagation(a,h):\n",
        "\n",
        "  grads_W = [0] * (no_of_layers + 1)\n",
        "  grads_b = [0] * (no_of_layers + 1)\n",
        "  grads_h = [0] * (no_of_layers + 1)\n",
        "  grads_a = [0] * (no_of_layers + 1)\n",
        "  grads_a[-1] = -(Y - h[-1])\n",
        "\n",
        "\n",
        "  for k in range(no_of_layers, 0, -1):\n",
        "    # Compute Gradient WRT parameters\n",
        "    grads_W[k] = grads_a[k] @ h[k-1].T\n",
        "    grads_b[k] = grads_a[k][:]\n",
        "    # Compute Gradients WRT layer below;\n",
        "    grads_h[k-1] = W[k].T @ grads_a[k]\n",
        "    # Compute Gradients WRT layer below(pre-activation)\n",
        "    grads_a[k-1] = grads_h[k-1] * grad_sigmoid(a[k-1])\n",
        "\n",
        "  grads_W[0] = grads_a[0] @ X.T\n",
        "  grads_b[0] = grads_a[0]\n",
        "\n",
        "  return (grads_W, grads_b, grads_h, grads_a)\n"
      ],
      "metadata": {
        "id": "ehXI-FMObhOc"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training/Gradient Descent\n",
        "\n",
        "max_epochs = 10\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  (a,h) = forward_propagation(X)\n",
        "  (grads_W, grads_b, grads_h, grads_a) = backward_propagation(a, h)\n",
        "\n",
        "  for i in range(no_of_layers + 1):\n",
        "    W[i] = W[i] - grads_W[i]\n",
        "    B[i] = B[i] - grads_b[i]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tP3HQz43bkyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565fcd37-ae85-4431-b60b-f8ed4cc733fc"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 0.00083319],\n",
            "       [ 0.00141185],\n",
            "       [-0.00211219]]), array([[ 0.01838198],\n",
            "       [-0.01997644],\n",
            "       [-0.0038401 ]]), array([[ 0.23691422],\n",
            "       [ 0.33838847],\n",
            "       [-0.57530268]])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-d514b35535e9>:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  denominator = np.sum(np.exp(i) for i in x)\n"
          ]
        }
      ]
    }
  ]
}